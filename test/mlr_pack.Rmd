---
title: "MLR Package"
author: "Binod Jung Bogati"
date: "12/25/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load libraries

```{r}
#install.packages("mlr")

library(mlr)
```

## Get Data

```{r}
train <- read.csv("train_loan.csv", na.strings = c("", " ", NA))

test <- read.csv("test_loan.csv", na.strings = c("", " ", NA))

```

2. Exploring Data

```{r}
summarizeColumns(train)


```

### Checking skewness in variables

```{r}
hist(train$ApplicantIncome, breaks = 300, main = "Applicant Income Chart")
```

```{r}
hist(train$CoapplicantIncome, breaks = 100, main = "Coapplicant Income chart")
```

```{r}
boxplot(train$ApplicantIncome)
```

```{r}
train$Credit_History <- as.factor(train$Credit_History)

test$Credit_History <- as.factor(test$Credit_History)
```


```{r}
class(train$Credit_History)
```

```{r}
library(knitr)
kable(summary(train))

summary(train$Dependents)
```

In variable, Dependents which has level 3+ which shall be treated. It's quite simple to modify the name levels in a factor variable. It can be done as:

```{r}
# rename level of Dependents

levels(train$Dependents)[4] <- "3"
levels(test$Dependents)[4] <- "3"
```

3. Missing Value Imputation

The impute() is easy because we don't have to specify each variable name to impute. Also, it treats the variables based on their classes.

```{r}
# impute missing values by mean and mode

imp <- impute(train, classes = list(factor = imputeMode(), integer = imputeMean()), dummy.classes = c("integer", "factor"), dummy.type = "numeric")

imp1 <- impute(test, classes = list(factor = imputeMode(), integer = imputeMean()), dummy.classes = c("integer", "factor"), dummy.type = "numeric")

```

The $data attribute imp function contains imputed data.

```{r}
imp_train <- imp$data

imp_test <- imp1$data
```

```{r}
summarizeColumns(imp_train)
summarizeColumns(imp_test)
```

```{r eval=FALSE, include=FALSE}
rpart_imp <- impute(train, target = "Loan_Status",
classes = list(numeric = imputeLearner(makeLearner("regr.rpart")),
factor = imputeLearner(makeLearner("classif.rpart"))),
dummy.classes = c("numeric","factor"),
dummy.type = "numeric")
```

4. Feature Engineering

Feature Engineering is interesting part of predictive modeling. So, feature engineering has two aspects: Feature Transformation and Feature Creation. 

First remove outliers from ApplicantIncome, CoapplicantIncome, LoanAmount.

```{r}
# for train data set

cd <- capLargeValues(imp_train, target = "Loan_Status", cols = c("ApplicantIncome"), threshold = 40000)

cd <- capLargeValues(cd, target = "Loan_Status", cols = c("CoapplicantIncome"), threshold = 23000)

cd <- capLargeValues(cd, target = "Loan_Status", cols = c("LoanAmount"), threshold = 520)


# rename train data as cd_train

cd_train <- cd

# add a dummy Loan_Status column in test data

imp_test$Loan_Status <- sample(0:1, size = 367, replace = T)


cde <- capLargeValues(imp_test, target = "Loan_Status", cols = c("ApplicantIncome"), threshold = 33000)

cde <- capLargeValues(cde, target = "Loan_Status", cols = c("CoapplicantIncome"), threshold = 16000)

cde <- capLargeValues(cde, target = "Loan_Status", cols = c("LoanAmount"), threshold = 470)

# renaming test data

cd_test <- cde

```














